from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering #, pipeline
import streamlit as st


summarization_tokenizer = AutoTokenizer.from_pretrained("Blaise-g/longt5_tglobal_large_sumpubmed")

summarization_model = AutoModelForSeq2SeqLM.from_pretrained("Blaise-g/longt5_tglobal_large_sumpubmed") 

#qa_tokenizer = AutoTokenizer.from_pretrained("sultan/BioM-ELECTRA-Large-SQuAD2-BioASQ8B")

#qa_model = AutoModelForQuestionAnswering.from_pretrained("sultan/BioM-ELECTRA-Large-SQuAD2-BioASQ8B")

#question_answer = pipeline("question-answering", model=qa_model, tokenizer = qa_tokenizer)

@st.cache

def summarize(note):
    input_ids = summarization_tokenizer.encode("summarize: " + note, return_tensors="pt", add_special_tokens=True)
    generated_ids = summarization_model .generate(input_ids=input_ids,num_beams=5,max_length=15000,repetition_penalty=2.5,length_penalty=0.75,early_stopping=True,num_return_sequences=3)
    preds = [summarization_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]
    return preds[0]


st.title('Machine Learning Based Clinical Summarization')
st.header('Enter Sample Medical Note:')
note = st.text_area("Enter some text ðŸ‘‡")

if st.button('Generate Summary'):
    summary = summarize(note)
    st.success(summary)



